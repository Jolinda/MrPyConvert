{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eb4889d",
   "metadata": {},
   "source": [
    "# Converting DICOMS to BIDS on talapas\n",
    "\n",
    "In this tutorial we'll use mrpyconvert and dcm2niix to convert some dicom files to BIDS format on talapas. mrpyconvert takes advantage of the way DICOMS are organized subject & series, and generates scripts that call dcm2niix.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db66e885-baa8-40ed-ac88-cc8f32414d9c",
   "metadata": {},
   "source": [
    "## Jupyter notebook on talapas\n",
    "As of this writing, the default python on talapas is out of date. This will hopefully change, but for now I recommend using an alternate conda environment. You can do that on onDemand by choosing the server Jupyter Notebook (Python3/TensorFlow/PyTorch), and the alternate conda environment jupyterlab-tf-plus-20220927."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6934aec9-9265-4ecd-b725-3192eb0159ee",
   "metadata": {},
   "source": [
    "## Installing mrpyconvert\n",
    "The source code for mrpyconvert is on github at: https://github.com/Jolinda/mrpyconvert.git. You can install it on using pip. If you are in a jupyter notebook, you can run bash commands by starting the cell with %%bash. Install mrpyconvert like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9885795c-ba2e-4c69-ab37-90992d65c188",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mrpyconvert in /gpfs/home/jolinda/.local/lib/python3.10/site-packages (0.1.4)\n",
      "Collecting mrpyconvert\n",
      "  Using cached mrpyconvert-0.1.18-py3-none-any.whl (8.4 kB)\n",
      "Collecting dcm2niix<2.0.0,>=1.0.2022\n",
      "  Using cached dcm2niix-1.0.20220715.tar.gz (451 kB)\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: pydicom<3.0,>=2.2 in /gpfs/packages/miniconda/20190102/envs/jupyterlab-tf-plus-20220927/lib/python3.10/site-packages (from mrpyconvert) (2.3.0)\n",
      "Requirement already satisfied: packaging in /gpfs/packages/miniconda/20190102/envs/jupyterlab-tf-plus-20220927/lib/python3.10/site-packages (from mrpyconvert) (21.3)\n",
      "Collecting miutil[web]\n",
      "  Using cached miutil-0.12.0-py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /gpfs/packages/miniconda/20190102/envs/jupyterlab-tf-plus-20220927/lib/python3.10/site-packages (from packaging->mrpyconvert) (3.0.9)\n",
      "Requirement already satisfied: tqdm>=4.40.0 in /gpfs/packages/miniconda/20190102/envs/jupyterlab-tf-plus-20220927/lib/python3.10/site-packages (from miutil[web]->dcm2niix<2.0.0,>=1.0.2022->mrpyconvert) (4.64.1)\n",
      "Requirement already satisfied: requests in /gpfs/packages/miniconda/20190102/envs/jupyterlab-tf-plus-20220927/lib/python3.10/site-packages (from miutil[web]->dcm2niix<2.0.0,>=1.0.2022->mrpyconvert) (2.28.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /gpfs/packages/miniconda/20190102/envs/jupyterlab-tf-plus-20220927/lib/python3.10/site-packages (from requests->miutil[web]->dcm2niix<2.0.0,>=1.0.2022->mrpyconvert) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /gpfs/packages/miniconda/20190102/envs/jupyterlab-tf-plus-20220927/lib/python3.10/site-packages (from requests->miutil[web]->dcm2niix<2.0.0,>=1.0.2022->mrpyconvert) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /gpfs/packages/miniconda/20190102/envs/jupyterlab-tf-plus-20220927/lib/python3.10/site-packages (from requests->miutil[web]->dcm2niix<2.0.0,>=1.0.2022->mrpyconvert) (1.26.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /gpfs/packages/miniconda/20190102/envs/jupyterlab-tf-plus-20220927/lib/python3.10/site-packages (from requests->miutil[web]->dcm2niix<2.0.0,>=1.0.2022->mrpyconvert) (2022.9.24)\n",
      "Building wheels for collected packages: dcm2niix\n",
      "  Building wheel for dcm2niix (pyproject.toml): started\n",
      "  Building wheel for dcm2niix (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for dcm2niix: filename=dcm2niix-1.0.20220715-cp310-cp310-linux_x86_64.whl size=583178 sha256=170a607556881b02f37ff5708c0aab6f12f2ca945e7c93318ff6e32716f3504b\n",
      "  Stored in directory: /gpfs/home/jolinda/.cache/pip/wheels/88/8d/9b/5ea20c0451a1acddef585757be7dfec121ee076e58503b267c\n",
      "Successfully built dcm2niix\n",
      "Installing collected packages: miutil, dcm2niix, mrpyconvert\n",
      "  Attempting uninstall: mrpyconvert\n",
      "    Found existing installation: mrpyconvert 0.1.4\n",
      "    Uninstalling mrpyconvert-0.1.4:\n",
      "      Successfully uninstalled mrpyconvert-0.1.4\n",
      "Successfully installed dcm2niix-1.0.20220715 miutil-0.12.0 mrpyconvert-0.1.18\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "python3 -m pip install --upgrade --user mrpyconvert"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ed1341-9dc6-4d32-9f86-53a86451c34b",
   "metadata": {},
   "source": [
    "This installs it locally for your use only, and will upgrade it if it is already installed. This will also install a local version of dcm2niix if one is not found. If you are on talapas, this version will likely be more current than any available as environment modules. \n",
    "\n",
    "You may get some warnings that scripts have been installed in a path that is not on PATH. There should be a file named .bashrc in your home directory on talapas. Edit it (eg, nano .bashrc) and add the line:\n",
    "```\n",
    "export PATH=$PATH:$HOME/.local/bin\n",
    "```\n",
    "At one point .bashrc did not get sourced for ondemand sessions -- I don't know if this is still true, but you may also need to create a file called .bashrc-ondemand in your home directory. That file should simply source .bashrc:\n",
    "```\n",
    "if [ f .bashrc ]; then\n",
    "  . .bashrc\n",
    "fi\n",
    "```\n",
    "Make sure that's in the .bashrc-ondemand file and not the .bashrc file, otherwise you'll get stuck in an infinite loop. You'll need to start a new session for those changes to take effect.\n",
    "\n",
    "### jq\n",
    "Some actions of mrpyconvert require the tool jq for editing json files on the command line. If you are on talapas, you can load it via an environment module (I'll show examples of how to include this). If you are running this somewhere else, get jq at https://jqlang.github.io/jq/."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08eaefad",
   "metadata": {},
   "source": [
    "## Example 1: single subject"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460299cd",
   "metadata": {},
   "source": [
    "For this tutorial we'll be using some data from the LCNI repository. This data is available to all University of Oregon researchers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "978d4ee7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AEPET2\tDEV  DIPPER  REV_examples  Round_Robin\n"
     ]
    }
   ],
   "source": [
    "!ls /projects/lcni/dcm/repository/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfbbc927",
   "metadata": {},
   "source": [
    "For our first example, we'll use the DIPPER study. First we're going to create some paths to the directories we're going to use. Substitute your own pirg for \"lcni\" in the output path. If you aren't familiar with the pathlib module, I recommend taking a moment to familiarize yourself with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ed75aa6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "repo = pathlib.Path('/projects/lcni/dcm/repository/')\n",
    "dipper_path = repo / 'DIPPER'\n",
    "output_path = pathlib.Path.home() / 'lcni' / 'bids_tutorial'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b13ffc",
   "metadata": {},
   "source": [
    "Next we need to create a Converter object and set the output path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "74e4d178",
   "metadata": {},
   "outputs": [],
   "source": [
    "import mrpyconvert\n",
    "converter = mrpyconvert.Converter()\n",
    "converter.set_bids_path(output_path / 'dipper')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc14e1c",
   "metadata": {},
   "source": [
    "We can use the converter to inspect the contents of the input directory before we add the files to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d2638fa2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 study for 1 subject found.\n",
      "Subjects: DIPPER_007\n",
      "AAHead_Scout_32ch-head-coil\n",
      "AAHead_Scout_32ch-head-coil_MPR_cor\n",
      "AAHead_Scout_32ch-head-coil_MPR_sag\n",
      "AAHead_Scout_32ch-head-coil_MPR_tra\n",
      "DIPPER_1\n",
      "DIPPER_2\n",
      "DIPPER_3\n",
      "DIPPER_4\n",
      "DIPPER_5\n",
      "DIPPER_6\n",
      "DIPPER_7\n",
      "DIPPER_8\n",
      "DIPPER_9\n",
      "mprage_p2_ND_defaced\n",
      "mprage_p2_defaced\n",
      "se_epi_mb3_g2_2mm_ap\n",
      "se_epi_mb3_g2_2mm_pa\n"
     ]
    }
   ],
   "source": [
    "converter.inspect(dipper_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b48d2287",
   "metadata": {},
   "source": [
    "We have a single subject, with several functional runs, a structural, and an AP/PA epi pair for fieldmap generation. It's helpful to have an idea of what the final BIDS output should look like, and it should have filenames that look something like this:\n",
    "```\n",
    "anat/sub-007_T1w\n",
    "fmap/sub-007_dir-AP_epi\n",
    "fmap/sub-007_dir-PA_epi\n",
    "func/sub-007_task-dipper_run-1_bold\n",
    "func/sub-007_task-dipper_run-2_bold\n",
    "etc\n",
    "```\n",
    " \n",
    "We can go ahead and add all the dicoms to the converter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188a20ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.add_dicoms(dipper_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "217fc39a",
   "metadata": {},
   "source": [
    "mrpyconvert creates a mapping of series directories to final BIDS output. Each of these mappings is referred to as an \"entry\". We use the add_entry command to define them:\n",
    "```\n",
    "converter.add_entry(name, search, datatype, suffix, chain, json_entries)\n",
    "```\n",
    "name: a descriptive name (eg, 'mprage') that will also be the name of the generated script \n",
    "search: string to search for in the series description. Regular expressions are allowed.  \n",
    "datatype: BIDS data type ('anat', 'func', etc)  \n",
    "suffix: BIDS suffix ('T1w', 'bold', etc)  \n",
    "chain: dictionary of anything else that needs to go into the file name ({'run':'1', 'dir':'AP'}, etc)  \n",
    "json_fields: dictionary of additional fields that need to go into the .json sidecar file\n",
    "  \n",
    "We will start with the mprage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "95c0c404",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.add_entry('mprage', search='mprage_p2_defaced', datatype='anat', suffix='T1w')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abdd4361",
   "metadata": {},
   "source": [
    "By default, mrpyconvert will use the Patient Name frome the dicom file for the subject name: sub-{Patient Name}. In this case, there's a problem: the subject name has an underscore, and that's not allowed. So we need to set the subject name to something else, such as \"007\" or \"dipper007\". We do that with a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "60a26bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.set_names({'DIPPER_007':'007'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885aae1d",
   "metadata": {},
   "source": [
    "For a simple conversion like this, we can just tell the converter to convert everything. For bigger jobs you'll want to submit it to the grid. We'll come back to that later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ff37fab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting mprage\n"
     ]
    }
   ],
   "source": [
    "converter.convert()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e564702e",
   "metadata": {},
   "source": [
    "What does our output look like?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bb22c344",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolinda/lcni/bids_tutorial/dipper\n",
      "`-- sub-007\n",
      "    `-- anat\n",
      "        |-- sub-007_T1w.json\n",
      "        `-- sub-007_T1w.nii.gz\n",
      "\n",
      "2 directories, 2 files\n"
     ]
    }
   ],
   "source": [
    "!tree {output_path}/dipper"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac41ab3d",
   "metadata": {},
   "source": [
    "To add the field maps, we'll need to use the 'chain' and 'json_fields' keyword arguments. Field maps require an identifier that we'll refer to in the functional .json files to indicate which data goes together (you still need to do this even if there's only one field map). We'll use a wildcard in the search string this time.  \".\" will match any single character, \".*\" matches any number of characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "31d67969",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.add_entry('se_epi_ap', search='se_epi_.*_ap', datatype='fmap', suffix='epi', chain = {'dir':'AP'},\n",
    "                    json_fields={'B0FieldIdentifier': 'fieldmap'})\n",
    "converter.add_entry('se_epi_pa', search='se_epi_.*_pa', datatype='fmap', suffix='epi', chain = {'dir':'PA'},\n",
    "                    json_fields={'B0FieldIdentifier': 'fieldmap'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a76777",
   "metadata": {},
   "source": [
    "Finally we have the functional runs. Here we need to add the 'run' identifier. We *could* explicitly map each run individually, or we could let mrpyconvert number them consecutively using the 'autorun' option. That's not always a good idea (imagine you have a session where you had to stop and restart a run), but it will work fine with this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05124c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.add_entry('tasks', search='DIPPER_.', datatype='func', suffix='bold', autorun=True, chain={'task':'dipper'},\n",
    "                    json_fields={'TaskName': 'dipper', 'B0FieldSource': 'fieldmap'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b749e1a8",
   "metadata": {},
   "source": [
    "Normally you'll probably want to generate scripts that can be submitted to the cluster, rather than converting using the \"convert\" command. We can generate bash scripts, or scripts already formatted for SLURM. Here's what that looks like. I'm using the options \"script path\" and \"script extension\" arguments; otherwise they're in the current working directory with no extension. Each entry generates one script. I'm also including the module load jq command using the addition_commands argument. Anything in this list will be run before dcm2niix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "49930184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('dipper_scripts/mprage.sh'),\n",
       " PosixPath('dipper_scripts/se_epi_ap.sh'),\n",
       " PosixPath('dipper_scripts/se_epi_pa.sh'),\n",
       " PosixPath('dipper_scripts/tasks.sh')]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.generate_scripts(script_path='dipper_scripts', script_ext='.sh', additional_commands=['module load jq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37acab9a",
   "metadata": {},
   "source": [
    "Here's what the script looks like for the mprage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e7d4b2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "module load jq\n",
      "\n",
      "\n",
      "dicom_path=/gpfs/projects/lcni/dcm/repository/DIPPER/DIPPER_007_20191010_135619\n",
      "bids_path=/gpfs/projects/lcni/jolinda/bids_tutorial/dipper\n",
      "names=(007)\n",
      "input_dirs=(\"Series_1017_mprage_p2_defaced\")\n",
      "\n",
      "\n",
      "for i in \"${!names[@]}\"; do\n",
      "  name=${names[$i]}\n",
      "  input_dir=${input_dirs[$i]}\n",
      "  mkdir --parents \"${bids_path}/sub-${name}/anat\"\n",
      "  dcmoutput=$(dcm2niix -ba n -l o -o \"${bids_path}/sub-${name}/anat\" -f \"sub-${name}_T1w\"  ${dicom_path}/${input_dir})\n",
      "  echo \"${dcmoutput}\"\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "!cat dipper_scripts/mprage.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bb2b20f",
   "metadata": {},
   "source": [
    "If you plan to submit the job to slurm, you can have it output a file ready to submit using sbatch. You might need additional commands in the slurm file to define your pirg, partition, etc, if you haven't set these as environment variables. You'll also need to load the jq module (the mprage script doesn't use it, but the others do)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1372a777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('dipper_scripts/mprage.srun'),\n",
       " PosixPath('dipper_scripts/tasks.srun'),\n",
       " PosixPath('dipper_scripts/se_epi_ap.srun'),\n",
       " PosixPath('dipper_scripts/se_epi_pa.srun')]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.generate_scripts(script_path='dipper_scripts', script_ext='.srun', slurm=True, additional_commands=['#SBATCH --account=lcni', 'module load jq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "5036e4ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#SBATCH --job-name=tasks\n",
      "#SBATCH --array=0-8\n",
      "#SBATCH --account=lcni\n",
      "module load jq\n",
      "\n",
      "\n",
      "dicom_path=/gpfs/projects/lcni/dcm/repository/DIPPER/DIPPER_007_20191010_135619\n",
      "bids_path=/gpfs/projects/lcni/jolinda/bids_tutorial/dipper\n",
      "names=(007 007 007 007 007 007 007 007 007)\n",
      "runs=(1 2 3 4 5 6 7 8 9)\n",
      "input_dirs=(\"Series_7_DIPPER_1\" \\\n",
      "            \"Series_8_DIPPER_2\" \\\n",
      "            \"Series_9_DIPPER_3\" \\\n",
      "            \"Series_10_DIPPER_4\" \\\n",
      "            \"Series_11_DIPPER_5\" \\\n",
      "            \"Series_12_DIPPER_6\" \\\n",
      "            \"Series_13_DIPPER_7\" \\\n",
      "            \"Series_14_DIPPER_8\" \\\n",
      "            \"Series_15_DIPPER_9\")\n",
      "\n",
      "\n",
      "name=${names[$SLURM_ARRAY_TASK_ID]}\n",
      "input_dir=${input_dirs[$SLURM_ARRAY_TASK_ID]}\n",
      "run=${runs[$SLURM_ARRAY_TASK_ID]}\n",
      "mkdir --parents \"${bids_path}/sub-${name}/func\"\n",
      "dcmoutput=$(dcm2niix -ba n -l o -o \"${bids_path}/sub-${name}/func\" -f \"sub-${name}_task-dipper_run-${run}_bold\"  ${dicom_path}/${input_dir})\n",
      "echo \"${dcmoutput}\"\n",
      "\n",
      "# get names of converted files\n",
      "if grep -q Convert <<< ${dcmoutput}; then \n",
      "  tmparray=($(echo \"${dcmoutput}\" | grep Convert ))\n",
      "  output_files=()\n",
      "  for ((i=4; i<${#tmparray[@]}; i+=6)); do output_files+=(\"${tmparray[$i]}\"); done\n",
      "  for output_file in ${output_files[@]}; do\n",
      "    # add fields to json file(s)\n",
      "    jq '.TaskName = \"dipper\"|.B0FieldSource = \"fieldmap\"' ${output_file}.json > ${output_file}.tmp \n",
      "    mv ${output_file}.tmp ${output_file}.json\n",
      "  done\n",
      "fi\n"
     ]
    }
   ],
   "source": [
    "!cat dipper_scripts/tasks.srun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "53c4fbb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 26214472\n",
      "Submitted batch job 26214473\n",
      "Submitted batch job 26214474\n",
      "Submitted batch job 26214475\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for x in dipper_scripts/*.srun; do sbatch ${x}; done"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e329cd6",
   "metadata": {},
   "source": [
    "That was a bit lazy; I wound up converting the mprage twice. You'll see it below with an 'a' appended to the filename."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "d1361040",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolinda/lcni/bids_tutorial/dipper\n",
      "`-- sub-007\n",
      "    |-- anat\n",
      "    |   |-- sub-007_T1w.json\n",
      "    |   |-- sub-007_T1w.nii.gz\n",
      "    |   |-- sub-007_T1wa.json\n",
      "    |   `-- sub-007_T1wa.nii.gz\n",
      "    |-- fmap\n",
      "    |   |-- sub-007_dir-AP_epi.json\n",
      "    |   |-- sub-007_dir-AP_epi.nii.gz\n",
      "    |   |-- sub-007_dir-PA_epi.json\n",
      "    |   `-- sub-007_dir-PA_epi.nii.gz\n",
      "    `-- func\n",
      "        |-- sub-007_task-dipper_run-1_bold.json\n",
      "        |-- sub-007_task-dipper_run-1_bold.nii.gz\n",
      "        |-- sub-007_task-dipper_run-2_bold.json\n",
      "        |-- sub-007_task-dipper_run-2_bold.nii.gz\n",
      "        |-- sub-007_task-dipper_run-3_bold.json\n",
      "        |-- sub-007_task-dipper_run-3_bold.nii.gz\n",
      "        |-- sub-007_task-dipper_run-4_bold.json\n",
      "        |-- sub-007_task-dipper_run-4_bold.nii.gz\n",
      "        |-- sub-007_task-dipper_run-5_bold.json\n",
      "        |-- sub-007_task-dipper_run-5_bold.nii.gz\n",
      "        |-- sub-007_task-dipper_run-6_bold.json\n",
      "        |-- sub-007_task-dipper_run-6_bold.nii.gz\n",
      "        |-- sub-007_task-dipper_run-7_bold.json\n",
      "        |-- sub-007_task-dipper_run-7_bold.nii.gz\n",
      "        |-- sub-007_task-dipper_run-8_bold.json\n",
      "        |-- sub-007_task-dipper_run-8_bold.nii.gz\n",
      "        |-- sub-007_task-dipper_run-9_bold.json\n",
      "        `-- sub-007_task-dipper_run-9_bold.nii.gz\n",
      "\n",
      "4 directories, 26 files\n"
     ]
    }
   ],
   "source": [
    "!tree {bids_path}/dipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b90f1a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm {bids_path}/dipper/sub-007/anat/*a.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02e05ee3",
   "metadata": {},
   "source": [
    "## Example 2: multiple subjects, diffusion data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b87ccf2",
   "metadata": {},
   "source": [
    "Let's create a new converter and look at the Round Robin sample data. This time we'll add the dicoms, then inspect them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "728efb93",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = mrpyconvert.Converter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5ca91ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.add_dicoms(repo / 'Round_Robin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a32ecdb9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 studies for 3 subjects found.\n",
      "Subjects: G16_S01 G17_S01 G18_S02\n",
      "1EPI188\n",
      "2EPI188\n",
      "3EPI188\n",
      "4EPI188\n",
      "5EPI188\n",
      "AAHScout_32ch-head-coil\n",
      "AAHScout_32ch-head-coil_MPR_cor\n",
      "AAHScout_32ch-head-coil_MPR_sag\n",
      "AAHScout_32ch-head-coil_MPR_tra\n",
      "AP_fieldmap_se_epi_mb3_g2_2mm_ap\n",
      "EPI196\n",
      "LR_diff_m2p2_64_2mm_lr\n",
      "PA_fieldmap_se_epi_mb3_g2_2mm_pa\n",
      "RL_diff_m2p2_64_2mm_rl\n",
      "mprage_defaced\n"
     ]
    }
   ],
   "source": [
    "converter.inspect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e9fc9c",
   "metadata": {},
   "source": [
    "Each subject has six functional runs, an mprage, fieldmaps, and diffusion scans. We didn't get any warnings, so no subjects have duplicate runs. Once again, we have problematic subject names, so let's map those out first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a6e355d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.set_names({'G16_S01':'G16S01', 'G17_S01':'G17S01', 'G18_S02':'G18S02'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d24d2c1",
   "metadata": {},
   "source": [
    "We can do the mprage, field maps, and functionals similarly to the way we did the previous scans. I'm going to call EPI188 task \"A\" and EPI196 task \"B\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a62ae6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.add_entry('mprage', search='mprage_.*', datatype='anat', suffix='T1w')\n",
    "converter.add_entry('se_epi_ap', search='AP_fieldmap.*', datatype='fmap', suffix='epi', chain = {'dir':'AP'},\n",
    "                    json_fields={'B0FieldIdentifier': 'fieldmap'})\n",
    "converter.add_entry('se_epi_pa', search='PA_fieldmap.*', datatype='fmap', suffix='epi', chain = {'dir':'PA'},\n",
    "                    json_fields={'B0FieldIdentifier': 'fieldmap'})\n",
    "converter.add_entry('taskA', search='.EPI188', datatype='func', suffix='bold', autorun=True, chain={'task':'A'},\n",
    "                    json_fields={'TaskName': 'A', 'B0FieldSource': 'fieldmap'})\n",
    "converter.add_entry('taskB', search='EPI196', datatype='func', suffix='bold', chain={'task':'B'},\n",
    "                    json_fields={'TaskName': 'B', 'B0FieldSource': 'fieldmap'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8400d001",
   "metadata": {},
   "source": [
    "Diffusion scans are straightforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b680bf99",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.add_entry('dwi_LR', search='LR_diff.*', datatype='dwi', suffix = 'dwi', chain = {'dir':'LR'})\n",
    "converter.add_entry('dwi_RL', search='RL_diff.*', datatype='dwi', suffix = 'dwi', chain = {'dir':'RL'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "58e8c1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.set_bids_path(output_path / 'RoundRobin')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "460c049d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('RRscripts/mprage.srun'),\n",
       " PosixPath('RRscripts/se_epi_ap.srun'),\n",
       " PosixPath('RRscripts/se_epi_pa.srun'),\n",
       " PosixPath('RRscripts/taskA.srun'),\n",
       " PosixPath('RRscripts/taskB.srun'),\n",
       " PosixPath('RRscripts/dwi_LR.srun'),\n",
       " PosixPath('RRscripts/dwi_RL.srun')]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.generate_scripts(script_path='RRscripts', script_ext='.srun', slurm=True, additional_commands=['#SBATCH --account=lcni', 'module load jq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b0387d50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#SBATCH --job-name=dwi_LR\n",
      "#SBATCH --array=0-2\n",
      "#SBATCH --account=lcni\n",
      "module load jq\n",
      "\n",
      "\n",
      "dicom_path=/gpfs/projects/lcni/dcm/repository/Round_Robin\n",
      "bids_path=/gpfs/projects/lcni/jolinda/bids_tutorial/RoundRobin\n",
      "names=(G16S01 G17S01 G18S02)\n",
      "input_dirs=(\"G16_S01_20191111_103547/Series_15_LR_diff_m2p2_64_2mm_lr\" \\\n",
      "            \"G17_S01_20191218_091736/Series_15_LR_diff_m2p2_64_2mm_lr\" \\\n",
      "            \"G18_S02_20191121_132748/Series_15_LR_diff_m2p2_64_2mm_lr\")\n",
      "\n",
      "\n",
      "name=${names[$SLURM_ARRAY_TASK_ID]}\n",
      "input_dir=${input_dirs[$SLURM_ARRAY_TASK_ID]}\n",
      "mkdir --parents \"${bids_path}/sub-${name}/dwi\"\n",
      "dcmoutput=$(dcm2niix -ba n -l o -o \"${bids_path}/sub-${name}/dwi\" -f \"sub-${name}_dir-LR_dwi\"  ${dicom_path}/${input_dir})\n",
      "echo \"${dcmoutput}\"\n"
     ]
    }
   ],
   "source": [
    "!cat RRscripts/dwi_LR.srun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee083426",
   "metadata": {},
   "source": [
    "We can submit these scripts to slurm using sbatch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "0a8e8000",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 26214476\n",
      "Submitted batch job 26214477\n",
      "Submitted batch job 26214478\n",
      "Submitted batch job 26214479\n",
      "Submitted batch job 26214480\n",
      "Submitted batch job 26214481\n",
      "Submitted batch job 26214482\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "sbatch RRscripts/mprage.srun\n",
    "sbatch RRscripts/se_epi_ap.srun\n",
    "sbatch RRscripts/se_epi_pa.srun\n",
    "sbatch RRscripts/taskA.srun\n",
    "sbatch RRscripts/taskB.srun\n",
    "sbatch RRscripts/dwi_LR.srun\n",
    "sbatch RRscripts/dwi_RL.srun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46379bd4",
   "metadata": {},
   "source": [
    "We can use sacct to track whether the job is finished"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3421338a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JobID             State ExitCode \n",
      "------------ ---------- -------- \n",
      "26214479_0    COMPLETED      0:0 \n",
      "26214479_0.+  COMPLETED      0:0 \n",
      "26214479_0.+  COMPLETED      0:0 \n",
      "26214479_1    COMPLETED      0:0 \n",
      "26214479_1.+  COMPLETED      0:0 \n",
      "26214479_1.+  COMPLETED      0:0 \n",
      "26214479_2    COMPLETED      0:0 \n",
      "26214479_2.+  COMPLETED      0:0 \n",
      "26214479_2.+  COMPLETED      0:0 \n",
      "26214479_3    COMPLETED      0:0 \n",
      "26214479_3.+  COMPLETED      0:0 \n",
      "26214479_3.+  COMPLETED      0:0 \n",
      "26214479_4    COMPLETED      0:0 \n",
      "26214479_4.+  COMPLETED      0:0 \n",
      "26214479_4.+  COMPLETED      0:0 \n",
      "26214479_5    COMPLETED      0:0 \n",
      "26214479_5.+  COMPLETED      0:0 \n",
      "26214479_5.+  COMPLETED      0:0 \n",
      "26214479_6    COMPLETED      0:0 \n",
      "26214479_6.+  COMPLETED      0:0 \n",
      "26214479_6.+  COMPLETED      0:0 \n",
      "26214479_7    COMPLETED      0:0 \n",
      "26214479_7.+  COMPLETED      0:0 \n",
      "26214479_7.+  COMPLETED      0:0 \n",
      "26214479_8    COMPLETED      0:0 \n",
      "26214479_8.+  COMPLETED      0:0 \n",
      "26214479_8.+  COMPLETED      0:0 \n",
      "26214479_9    COMPLETED      0:0 \n",
      "26214479_9.+  COMPLETED      0:0 \n",
      "26214479_9.+  COMPLETED      0:0 \n",
      "26214479_10   COMPLETED      0:0 \n",
      "26214479_10+  COMPLETED      0:0 \n",
      "26214479_10+  COMPLETED      0:0 \n",
      "26214479_11   COMPLETED      0:0 \n",
      "26214479_11+  COMPLETED      0:0 \n",
      "26214479_11+  COMPLETED      0:0 \n",
      "26214479_12   COMPLETED      0:0 \n",
      "26214479_12+  COMPLETED      0:0 \n",
      "26214479_12+  COMPLETED      0:0 \n",
      "26214479_13   COMPLETED      0:0 \n",
      "26214479_13+  COMPLETED      0:0 \n",
      "26214479_13+  COMPLETED      0:0 \n",
      "26214479_14   COMPLETED      0:0 \n",
      "26214479_14+  COMPLETED      0:0 \n",
      "26214479_14+  COMPLETED      0:0 \n"
     ]
    }
   ],
   "source": [
    "!sacct --name taskA -b"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85a342e",
   "metadata": {},
   "source": [
    "Anything that would have printed to the command line goes to slurm*.out and slurm*.err"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2a3c66fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chris Rorden's dcm2niiX version v1.0.20220505  GCC4.8.5 x86-64 (64-bit Linux)\n",
      "Found 188 DICOM file(s)\n",
      "Convert 188 DICOM as /gpfs/projects/lcni/jolinda/bids_tutorial/RoundRobin/sub-G16S01/func/sub-G16S01_task-A_run-1_bold (104x104x72x188)\n",
      "Compress: \"/bin/pigz\" -b 960 -n -f -6 \"/gpfs/projects/lcni/jolinda/bids_tutorial/RoundRobin/sub-G16S01/func/sub-G16S01_task-A_run-1_bold.nii\"\n",
      "Conversion required 29.277022 seconds (1.020000 for core code).\n"
     ]
    }
   ],
   "source": [
    "!cat slurm-26214479_0.out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0f70a",
   "metadata": {},
   "source": [
    "We can check whether we have a valid bids data set by uploading the files to the bids validator at https://bids-standard.github.io/bids-validator. It will fail and will tell us exactly what's wrong: a missing dataset description json file. mrpyconvert can make one, and can also make the optional participants.tsv and .json files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e49fbc20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolinda/lcni/bids_tutorial/RoundRobin/dataset_description.json\n",
      "/home/jolinda/lcni/bids_tutorial/RoundRobin/participants.tsv\n",
      "/home/jolinda/lcni/bids_tutorial/RoundRobin/participants.json\n"
     ]
    }
   ],
   "source": [
    "converter.write_description_file()\n",
    "converter.write_participants_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "bf4a9f89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "participant_id\tsex\tage\n",
      "sub-G18S02\tF\t18\n",
      "sub-G17S01\tF\t28\n",
      "sub-G16S01\tF\t21\n"
     ]
    }
   ],
   "source": [
    "cat /home/jolinda/lcni/bids_tutorial/RoundRobin/participants.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0571b110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"age\": {\n",
      "    \"Description\": \"age of participant\",\n",
      "    \"Units\": \"years\"\n",
      "  },\n",
      "  \"sex\": {\n",
      "    \"Description\": \"sex of participant\",\n",
      "    \"Levels\": {\n",
      "      \"M\": \"male\",\n",
      "      \"F\": \"female\",\n",
      "      \"O\": \"other\"\n",
      "    }\n",
      "  }\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load jq\n",
    "jq . /home/jolinda/lcni/bids_tutorial/RoundRobin/participants.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3b1dca0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Name\": \"RoundRobin\",\n",
      "  \"BIDSVersion\": \"1.8.0\",\n",
      "  \"GeneratedBy\": [\n",
      "    {\n",
      "      \"Name\": \"dcm2niix\",\n",
      "      \"version\": \"1.0.20220505\"\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"mrpyconvert\",\n",
      "      \"version\": \"0.1.4\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load jq\n",
    "jq . /home/jolinda/lcni/bids_tutorial/RoundRobin/dataset_description.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a915c994",
   "metadata": {},
   "source": [
    "This time we pass the bids validator test, with some warnings. One of them is that we are missing an author list in the dataset_description file. We can add that and anything else the file needs when we create the file (we can also set a different name for the project if we don't want to use the bids directory name)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b5ed126c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolinda/lcni/bids_tutorial/RoundRobin/dataset_description.json\n"
     ]
    }
   ],
   "source": [
    "converter.write_description_file(json_fields={'Name':'Round Robin', 'Authors':['Jolinda Smith', 'Peter Parker']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "2a3b4a46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"Name\": \"Round Robin\",\n",
      "  \"Authors\": [\n",
      "    \"Jolinda Smith\",\n",
      "    \"Peter Parker\"\n",
      "  ],\n",
      "  \"BIDSVersion\": \"1.8.0\",\n",
      "  \"GeneratedBy\": [\n",
      "    {\n",
      "      \"Name\": \"dcm2niix\",\n",
      "      \"version\": \"1.0.20220505\"\n",
      "    },\n",
      "    {\n",
      "      \"Name\": \"mrpyconvert\",\n",
      "      \"version\": \"0.1.16\"\n",
      "    }\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "module load jq\n",
    "jq . /home/jolinda/lcni/bids_tutorial/RoundRobin/dataset_description.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "67745afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolinda/lcni/bids_tutorial/RoundRobin\n",
      "|-- dataset_description.json\n",
      "|-- participants.json\n",
      "|-- participants.tsv\n",
      "|-- sub-G16S01\n",
      "|   |-- anat\n",
      "|   |   |-- sub-G16S01_T1w.json\n",
      "|   |   `-- sub-G16S01_T1w.nii.gz\n",
      "|   |-- dwi\n",
      "|   |   |-- sub-G16S01_dir-LR_dwi.bval\n",
      "|   |   |-- sub-G16S01_dir-LR_dwi.bvec\n",
      "|   |   |-- sub-G16S01_dir-LR_dwi.json\n",
      "|   |   |-- sub-G16S01_dir-LR_dwi.nii.gz\n",
      "|   |   |-- sub-G16S01_dir-RL_dwi.bval\n",
      "|   |   |-- sub-G16S01_dir-RL_dwi.bvec\n",
      "|   |   |-- sub-G16S01_dir-RL_dwi.json\n",
      "|   |   `-- sub-G16S01_dir-RL_dwi.nii.gz\n",
      "|   |-- fmap\n",
      "|   |   |-- sub-G16S01_dir-AP_epi.json\n",
      "|   |   |-- sub-G16S01_dir-AP_epi.nii.gz\n",
      "|   |   |-- sub-G16S01_dir-PA_epi.json\n",
      "|   |   `-- sub-G16S01_dir-PA_epi.nii.gz\n",
      "|   `-- func\n",
      "|       |-- sub-G16S01_task-A_run-1_bold.json\n",
      "|       |-- sub-G16S01_task-A_run-1_bold.nii.gz\n",
      "|       |-- sub-G16S01_task-A_run-2_bold.json\n",
      "|       |-- sub-G16S01_task-A_run-2_bold.nii.gz\n",
      "|       |-- sub-G16S01_task-A_run-3_bold.json\n",
      "|       |-- sub-G16S01_task-A_run-3_bold.nii.gz\n",
      "|       |-- sub-G16S01_task-A_run-4_bold.json\n",
      "|       |-- sub-G16S01_task-A_run-4_bold.nii.gz\n",
      "|       |-- sub-G16S01_task-A_run-5_bold.json\n",
      "|       |-- sub-G16S01_task-A_run-5_bold.nii.gz\n",
      "|       |-- sub-G16S01_task-B_bold.json\n",
      "|       `-- sub-G16S01_task-B_bold.nii.gz\n",
      "|-- sub-G17S01\n",
      "|   |-- anat\n",
      "|   |   |-- sub-G17S01_T1w.json\n",
      "|   |   `-- sub-G17S01_T1w.nii.gz\n",
      "|   |-- dwi\n",
      "|   |   |-- sub-G17S01_dir-LR_dwi.bval\n",
      "|   |   |-- sub-G17S01_dir-LR_dwi.bvec\n",
      "|   |   |-- sub-G17S01_dir-LR_dwi.json\n",
      "|   |   |-- sub-G17S01_dir-LR_dwi.nii.gz\n",
      "|   |   |-- sub-G17S01_dir-RL_dwi.bval\n",
      "|   |   |-- sub-G17S01_dir-RL_dwi.bvec\n",
      "|   |   |-- sub-G17S01_dir-RL_dwi.json\n",
      "|   |   `-- sub-G17S01_dir-RL_dwi.nii.gz\n",
      "|   |-- fmap\n",
      "|   |   |-- sub-G17S01_dir-AP_epi.json\n",
      "|   |   |-- sub-G17S01_dir-AP_epi.nii.gz\n",
      "|   |   |-- sub-G17S01_dir-PA_epi.json\n",
      "|   |   `-- sub-G17S01_dir-PA_epi.nii.gz\n",
      "|   `-- func\n",
      "|       |-- sub-G17S01_task-A_run-1_bold.json\n",
      "|       |-- sub-G17S01_task-A_run-1_bold.nii.gz\n",
      "|       |-- sub-G17S01_task-A_run-2_bold.json\n",
      "|       |-- sub-G17S01_task-A_run-2_bold.nii.gz\n",
      "|       |-- sub-G17S01_task-A_run-3_bold.json\n",
      "|       |-- sub-G17S01_task-A_run-3_bold.nii.gz\n",
      "|       |-- sub-G17S01_task-A_run-4_bold.json\n",
      "|       |-- sub-G17S01_task-A_run-4_bold.nii.gz\n",
      "|       |-- sub-G17S01_task-A_run-5_bold.json\n",
      "|       |-- sub-G17S01_task-A_run-5_bold.nii.gz\n",
      "|       |-- sub-G17S01_task-B_bold.json\n",
      "|       `-- sub-G17S01_task-B_bold.nii.gz\n",
      "`-- sub-G18S02\n",
      "    |-- anat\n",
      "    |   |-- sub-G18S02_T1w.json\n",
      "    |   `-- sub-G18S02_T1w.nii.gz\n",
      "    |-- dwi\n",
      "    |   |-- sub-G18S02_dir-LR_dwi.bval\n",
      "    |   |-- sub-G18S02_dir-LR_dwi.bvec\n",
      "    |   |-- sub-G18S02_dir-LR_dwi.json\n",
      "    |   |-- sub-G18S02_dir-LR_dwi.nii.gz\n",
      "    |   |-- sub-G18S02_dir-RL_dwi.bval\n",
      "    |   |-- sub-G18S02_dir-RL_dwi.bvec\n",
      "    |   |-- sub-G18S02_dir-RL_dwi.json\n",
      "    |   `-- sub-G18S02_dir-RL_dwi.nii.gz\n",
      "    |-- fmap\n",
      "    |   |-- sub-G18S02_dir-AP_epi.json\n",
      "    |   |-- sub-G18S02_dir-AP_epi.nii.gz\n",
      "    |   |-- sub-G18S02_dir-PA_epi.json\n",
      "    |   `-- sub-G18S02_dir-PA_epi.nii.gz\n",
      "    `-- func\n",
      "        |-- sub-G18S02_task-A_run-1_bold.json\n",
      "        |-- sub-G18S02_task-A_run-1_bold.nii.gz\n",
      "        |-- sub-G18S02_task-A_run-2_bold.json\n",
      "        |-- sub-G18S02_task-A_run-2_bold.nii.gz\n",
      "        |-- sub-G18S02_task-A_run-3_bold.json\n",
      "        |-- sub-G18S02_task-A_run-3_bold.nii.gz\n",
      "        |-- sub-G18S02_task-A_run-4_bold.json\n",
      "        |-- sub-G18S02_task-A_run-4_bold.nii.gz\n",
      "        |-- sub-G18S02_task-A_run-5_bold.json\n",
      "        |-- sub-G18S02_task-A_run-5_bold.nii.gz\n",
      "        |-- sub-G18S02_task-B_bold.json\n",
      "        `-- sub-G18S02_task-B_bold.nii.gz\n",
      "\n",
      "15 directories, 81 files\n"
     ]
    }
   ],
   "source": [
    "!tree {output_path}/RoundRobin"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a25cbaec",
   "metadata": {},
   "source": [
    "## Example 3: multiple sessions, other fieldmaps, messy data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2d53258",
   "metadata": {},
   "source": [
    "Let's take a look at the REV example files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "4c9ba01b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 studies for 3 subjects found.\n",
      "Subjects: REV055 REV074 REV126\n",
      "AAHScout\n",
      "AAHScout_MPR_cor\n",
      "AAHScout_MPR_sag\n",
      "AAHScout_MPR_tra\n",
      "BART1_mb3_g2_2mm_te27\n",
      "BART2_mb3_g2_2mm_te27\n",
      "GNG1_mb3_g2_2mm_te27\n",
      "GNG2_mb3_g2_2mm_te27\n",
      "GNG3_mb3_g2_2mm_te27\n",
      "GNG4_mb3_g2_2mm_te27\n",
      "PhoenixZIPReport\n",
      "React1_mb3_g2_2mm_te27\n",
      "React2_mb3_g2_2mm_te27\n",
      "React3_mb3_g2_2mm_te27\n",
      "React4_mb3_g2_2mm_te27\n",
      "SST1_mb3_g2_2mm_te27\n",
      "SST2_mb3_g2_2mm_te27\n",
      "SST3_mb3_g2_2mm_te27\n",
      "SST4_mb3_g2_2mm_te27\n",
      "fieldmap1\n",
      "fieldmap2\n",
      "fieldmap3\n",
      "fieldmap4\n",
      "mprage1_MGH_p2_defaced\n",
      "mprage2_MGH_p2_defaced\n",
      "More than one copy of AAHScout for at least one study\n",
      "More than one copy of AAHScout_MPR_sag for at least one study\n",
      "More than one copy of AAHScout_MPR_cor for at least one study\n",
      "More than one copy of GNG3_mb3_g2_2mm_te27 for at least one study\n",
      "More than one copy of fieldmap1 for at least one study\n",
      "More than one copy of AAHScout_MPR_tra for at least one study\n",
      "More than one copy of fieldmap3 for at least one study\n",
      "More than one copy of fieldmap4 for at least one study\n",
      "More than one copy of GNG1_mb3_g2_2mm_te27 for at least one study\n",
      "More than one copy of fieldmap2 for at least one study\n",
      "More than one copy of BART1_mb3_g2_2mm_te27 for at least one study\n"
     ]
    }
   ],
   "source": [
    "converter.inspect(repo / 'REV_examples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e6a023d",
   "metadata": {},
   "source": [
    "Let's take a closer look at one subject to make sense of this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e69e7444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/projects/lcni/dcm/repository/REV_examples/REV055_20150811_135636:\n",
      "Series_1010_mprage1_MGH_p2_defaced  Series_3_AAHScout_MPR_cor\n",
      "Series_11_SST1_mb3_g2_2mm_te27\t    Series_4_AAHScout_MPR_tra\n",
      "Series_12_SST2_mb3_g2_2mm_te27\t    Series_5_GNG2_mb3_g2_2mm_te27\n",
      "Series_13_fieldmap2\t\t    Series_6_GNG1_mb3_g2_2mm_te27\n",
      "Series_14_fieldmap2\t\t    Series_7_fieldmap1\n",
      "Series_15_React1_mb3_g2_2mm_te27    Series_8_fieldmap1\n",
      "Series_16_React2_mb3_g2_2mm_te27    Series_99_PhoenixZIPReport\n",
      "Series_1_AAHScout\t\t    Series_9_BART1_mb3_g2_2mm_te27\n",
      "Series_2_AAHScout_MPR_sag\n",
      "\n",
      "/projects/lcni/dcm/repository/REV_examples/REV055_20150905_102727:\n",
      "Series_1010_mprage2_MGH_p2_defaced  Series_3_AAHScout_MPR_cor\n",
      "Series_11_GNG3_mb3_g2_2mm_te27\t    Series_4_AAHScout_MPR_tra\n",
      "Series_12_GNG4_mb3_g2_2mm_te27\t    Series_5_SST3_mb3_g2_2mm_te27\n",
      "Series_13_fieldmap4\t\t    Series_6_SST4_mb3_g2_2mm_te27\n",
      "Series_14_fieldmap4\t\t    Series_7_fieldmap3\n",
      "Series_15_React4_mb3_g2_2mm_te27    Series_8_fieldmap3\n",
      "Series_16_React3_mb3_g2_2mm_te27    Series_99_PhoenixZIPReport\n",
      "Series_1_AAHScout\t\t    Series_9_BART2_mb3_g2_2mm_te27\n",
      "Series_2_AAHScout_MPR_sag\n"
     ]
    }
   ],
   "source": [
    "!ls {repo}/REV_examples/REV055* "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e947e494",
   "metadata": {},
   "source": [
    "From here we can see that we have two sessions for each subject, and we can tell which session is which according to the name of the run. As long as we include the keyword 'ses' in add_entry, mrpyconvert will take care of organizing the output correctly. So we might do something like this:\n",
    "```\n",
    "converter.add_entry('GNG1', search='GNG1.*', datatype='func', suffix='bold', chain={'task': 'GNG', 'run': '1', 'ses': '1'})\n",
    "converter.add_entry('GNG2', search='GNG2.*', datatype='func', suffix='bold', chain={'task': 'GNG', 'run':'2', 'ses': '1'})\n",
    "converter.add_entry('GNG3', search='GNG3.*', datatype='func', suffix='bold', chain={'task': 'GNG', 'run': '1', 'ses': '2'})\n",
    "converter.add_entry('GNG4', search='GNG4.*', datatype='func', suffix='bold', chain={'task': 'GNG', 'run': '2', 'ses': '2'})\n",
    "```\n",
    "This would work great if our data was perfect! But check out the warning message from the inspection above:\n",
    "```\n",
    "More than one copy of GNG1_mb3_g2_2mm_te27 for at least one study\n",
    "```\n",
    "This means that at least one subject has two GNG1 scans in the same session. This is a problem -- we'll wind up with files named:\n",
    "```\n",
    "sub-XX_task-GNG_ses-1_run-1_bold\n",
    "sub-XX_task-GNG_ses-1_run-1_bolda\n",
    "```\n",
    "It's safer to just take the series numbers as the run numbers. We can do that by using the special formatting character '%s', which will be replaced by dcm2niix with the series number \n",
    "```\n",
    "converter.add_entry('GNG1', search='GNG1.*', datatype='func', suffix='bold', chain={'task': 'GNG', 'run': '%s', 'ses': '1'})\n",
    "converter.add_entry('GNG2', search='GNG2.*', datatype='func', suffix='bold', chain={'task': 'GNG', 'run':'%s', 'ses': '1'})\n",
    "converter.add_entry('GNG3', search='GNG3.*', datatype='func', suffix='bold', chain={'task': 'GNG', 'run': '%s', 'ses': '2'})\n",
    "converter.add_entry('GNG4', search='GNG4.*', datatype='func', suffix='bold', chain={'task': 'GNG', 'run': '%s', 'ses': '2'})\n",
    "```\n",
    "You can also have mrpyconvert automatically determine the session number for each subject, by using the relative dates of each study. That's what we'll do below. No matter what, you are going to have some cleanup work afterwards to remove/rename files if you don't clean up the dicom files before conversion.  \n",
    "\n",
    "Finally, there are the fieldmaps. These are the older phasediff/magnitude pairs. Unfortunately, the series names don't tell us which is which. For this special case, we can set the suffix to \"auto\" and let mrpyconvert figure it out.\n",
    "```\n",
    "converter.set_autosession(True)\n",
    "converter.add_entity('fieldmap.*', datatype='fmap', suffix='auto', chain={'run': '%s'})\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b50a3339",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter = mrpyconvert.Converter()\n",
    "converter.add_dicoms(repo / 'REV_examples')\n",
    "converter.set_bids_path(output_path / 'REV')\n",
    "converter.set_autosession(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24ebfb84",
   "metadata": {},
   "outputs": [],
   "source": [
    "converter.add_entry('React', search='React.*', datatype='func', suffix='bold', chain={'task': 'React', 'run': '%s'}, json_fields={'TaskName': 'React'})\n",
    "converter.add_entry('BART', search='BART.*', datatype='func', suffix='bold', chain={'task': 'BART', 'run': '%s'}, json_fields={'TaskName': 'BART'})\n",
    "converter.add_entry('SST', search='SST.*', datatype='func', suffix='bold', chain={'task': 'SST', 'run': '%s'}, json_fields={'TaskName': 'SST'})\n",
    "converter.add_entry('GNG', search='GNG.*', datatype='func', suffix='bold', chain={'task': 'GNG', 'run': '%s'}, json_fields={'TaskName': 'GNG'})\n",
    "converter.add_entry('mprage', search='mprage.*', datatype='anat', suffix='T1w', chain={'acq': 'mprage', 'run': '%s'})\n",
    "converter.add_entry('fieldmap', search='fieldmap.*', datatype='fmap', suffix='auto', chain={'run': '%s'})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "108cef72",
   "metadata": {},
   "source": [
    "I'm leaving out the json entries for the field maps (B0FieldSource and B0FieldIdentifier). With multiple field maps per subject, and some subjects having extra fieldmaps, I've decided to fix that later on a case by case basis. I've added \"acq\" to the mprage, but that's completely optional."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bae55a00",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('rev_scripts/React.srun'),\n",
       " PosixPath('rev_scripts/BART.srun'),\n",
       " PosixPath('rev_scripts/SST.srun'),\n",
       " PosixPath('rev_scripts/GNG.srun'),\n",
       " PosixPath('rev_scripts/mprage.srun'),\n",
       " PosixPath('rev_scripts/fieldmap.srun')]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "converter.generate_scripts(script_path='rev_scripts', script_ext='.srun', slurm=True, \n",
    "                           additional_commands=['#SBATCH --account=lcni', \n",
    "                                                '#SBATCH --output=rev_out/%x-%A_%a.out',\n",
    "                                                '#SBATCH --error=rev_out/%x-%A_%a.err',\n",
    "                                                '\\n',\n",
    "                                                'module load jq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "428a2fb3",
   "metadata": {},
   "source": [
    "Those additional commands will write the \"slurm-.out\" files files to a directory called \"rev_out\". This will keep our working directory a little cleaner. Dont' forget  to create this directory. There are several other useful commands you can add here, including one to email you when the job is finished. You can find them at https://slurm.schedmd.com/sbatch.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7eb7091",
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir rev_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fa103c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "#SBATCH --job-name=BART\n",
      "#SBATCH --array=0-7\n",
      "#SBATCH --account=lcni\n",
      "#SBATCH --output=rev_out/%x-%A_%a.out\n",
      "#SBATCH --error=rev_out/%x-%A_%a.err\n",
      "\n",
      "\n",
      "module load jq\n",
      "\n",
      "\n",
      "dicom_path=/gpfs/projects/lcni/dcm/repository/REV_examples\n",
      "bids_path=/gpfs/projects/lcni/jolinda/bids_tutorial/REV\n",
      "names=(REV055 REV055 REV074 REV074 REV126 REV126 REV126 REV126)\n",
      "sessions=(1 2 1 2 1 1 1 2)\n",
      "input_dirs=(\"REV055_20150811_135636/Series_9_BART1_mb3_g2_2mm_te27\" \\\n",
      "            \"REV055_20150905_102727/Series_9_BART2_mb3_g2_2mm_te27\" \\\n",
      "            \"REV074_20151006_100216/Series_5_BART1_mb3_g2_2mm_te27\" \\\n",
      "            \"REV074_20151110_151323/Series_5_BART2_mb3_g2_2mm_te27\" \\\n",
      "            \"REV126_20160304_130506/Series_5_BART1_mb3_g2_2mm_te27\" \\\n",
      "            \"REV126_20160304_130506/Series_6_BART1_mb3_g2_2mm_te27\" \\\n",
      "            \"REV126_20160304_130506/Series_7_BART1_mb3_g2_2mm_te27\" \\\n",
      "            \"REV126_20160407_150231/Series_15_BART2_mb3_g2_2mm_te27\")\n",
      "\n",
      "\n",
      "name=${names[$SLURM_ARRAY_TASK_ID]}\n",
      "input_dir=${input_dirs[$SLURM_ARRAY_TASK_ID]}\n",
      "session=${sessions[$SLURM_ARRAY_TASK_ID]}\n",
      "mkdir --parents \"${bids_path}/sub-${name}/ses-${session}/func\"\n",
      "dcmoutput=$(dcm2niix -ba n -l o -o \"${bids_path}/sub-${name}/ses-${session}/func\" -f \"sub-${name}_ses-${session}_task-BART_run-%s_bold\"  ${dicom_path}/${input_dir})\n",
      "echo \"${dcmoutput}\"\n",
      "\n",
      "# get names of converted files\n",
      "if grep -q Convert <<< ${dcmoutput}; then \n",
      "  tmparray=($(echo \"${dcmoutput}\" | grep Convert ))\n",
      "  output_files=()\n",
      "  for ((i=4; i<${#tmparray[@]}; i+=6)); do output_files+=(\"${tmparray[$i]}\"); done\n",
      "  for output_file in ${output_files[@]}; do\n",
      "    # add fields to json file(s)\n",
      "    jq '.TaskName = \"BART\"' ${output_file}.json > ${output_file}.tmp \n",
      "    mv ${output_file}.tmp ${output_file}.json\n",
      "  done\n",
      "fi\n"
     ]
    }
   ],
   "source": [
    "!cat rev_scripts/BART.srun"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cae43513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submitted batch job 26215831\n",
      "Submitted batch job 26215832\n",
      "Submitted batch job 26215833\n",
      "Submitted batch job 26215834\n",
      "Submitted batch job 26215835\n",
      "Submitted batch job 26215836\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "for x in rev_scripts/*.srun; do sbatch $x; done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5297484a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolinda/lcni/bids_tutorial/REV/dataset_description.json\n",
      "/home/jolinda/lcni/bids_tutorial/REV/participants.tsv\n",
      "/home/jolinda/lcni/bids_tutorial/REV/participants.json\n"
     ]
    }
   ],
   "source": [
    "converter.write_description_file()\n",
    "converter.write_participants_file()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7a202a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/jolinda/lcni/bids_tutorial/REV\n",
      "|-- dataset_description.json\n",
      "|-- participants.json\n",
      "|-- participants.tsv\n",
      "|-- sub-REV055\n",
      "|   |-- ses-1\n",
      "|   |   |-- anat\n",
      "|   |   |   |-- sub-REV055_ses-1_acq-mprage_run-1010_T1w.json\n",
      "|   |   |   `-- sub-REV055_ses-1_acq-mprage_run-1010_T1w.nii.gz\n",
      "|   |   |-- fmap\n",
      "|   |   |   |-- sub-REV055_ses-1_run-13_magnitude1.json\n",
      "|   |   |   |-- sub-REV055_ses-1_run-13_magnitude1.nii.gz\n",
      "|   |   |   |-- sub-REV055_ses-1_run-13_magnitude2.json\n",
      "|   |   |   |-- sub-REV055_ses-1_run-13_magnitude2.nii.gz\n",
      "|   |   |   |-- sub-REV055_ses-1_run-14_phasediff.json\n",
      "|   |   |   |-- sub-REV055_ses-1_run-14_phasediff.nii.gz\n",
      "|   |   |   |-- sub-REV055_ses-1_run-7_magnitude1.json\n",
      "|   |   |   |-- sub-REV055_ses-1_run-7_magnitude1.nii.gz\n",
      "|   |   |   |-- sub-REV055_ses-1_run-7_magnitude2.json\n",
      "|   |   |   |-- sub-REV055_ses-1_run-7_magnitude2.nii.gz\n",
      "|   |   |   |-- sub-REV055_ses-1_run-8_phasediff.json\n",
      "|   |   |   `-- sub-REV055_ses-1_run-8_phasediff.nii.gz\n",
      "|   |   `-- func\n",
      "|   |       |-- sub-REV055_ses-1_task-BART_run-9_bold.json\n",
      "|   |       |-- sub-REV055_ses-1_task-BART_run-9_bold.nii.gz\n",
      "|   |       |-- sub-REV055_ses-1_task-GNG_run-5_bold.json\n",
      "|   |       |-- sub-REV055_ses-1_task-GNG_run-5_bold.nii.gz\n",
      "|   |       |-- sub-REV055_ses-1_task-GNG_run-6_bold.json\n",
      "|   |       |-- sub-REV055_ses-1_task-GNG_run-6_bold.nii.gz\n",
      "|   |       |-- sub-REV055_ses-1_task-React_run-15_bold.json\n",
      "|   |       |-- sub-REV055_ses-1_task-React_run-15_bold.nii.gz\n",
      "|   |       |-- sub-REV055_ses-1_task-React_run-16_bold.json\n",
      "|   |       |-- sub-REV055_ses-1_task-React_run-16_bold.nii.gz\n",
      "|   |       |-- sub-REV055_ses-1_task-SST_run-11_bold.json\n",
      "|   |       |-- sub-REV055_ses-1_task-SST_run-11_bold.nii.gz\n",
      "|   |       |-- sub-REV055_ses-1_task-SST_run-12_bold.json\n",
      "|   |       `-- sub-REV055_ses-1_task-SST_run-12_bold.nii.gz\n",
      "|   `-- ses-2\n",
      "|       |-- anat\n",
      "|       |   |-- sub-REV055_ses-2_acq-mprage_run-1010_T1w.json\n",
      "|       |   `-- sub-REV055_ses-2_acq-mprage_run-1010_T1w.nii.gz\n",
      "|       |-- fmap\n",
      "|       |   |-- sub-REV055_ses-2_run-13_magnitude1.json\n",
      "|       |   |-- sub-REV055_ses-2_run-13_magnitude1.nii.gz\n",
      "|       |   |-- sub-REV055_ses-2_run-13_magnitude2.json\n",
      "|       |   |-- sub-REV055_ses-2_run-13_magnitude2.nii.gz\n",
      "|       |   |-- sub-REV055_ses-2_run-14_phasediff.json\n",
      "|       |   |-- sub-REV055_ses-2_run-14_phasediff.nii.gz\n",
      "|       |   |-- sub-REV055_ses-2_run-7_magnitude1.json\n",
      "|       |   |-- sub-REV055_ses-2_run-7_magnitude1.nii.gz\n",
      "|       |   |-- sub-REV055_ses-2_run-7_magnitude2.json\n",
      "|       |   |-- sub-REV055_ses-2_run-7_magnitude2.nii.gz\n",
      "|       |   |-- sub-REV055_ses-2_run-8_phasediff.json\n",
      "|       |   `-- sub-REV055_ses-2_run-8_phasediff.nii.gz\n",
      "|       `-- func\n",
      "|           |-- sub-REV055_ses-2_task-BART_run-9_bold.json\n",
      "|           |-- sub-REV055_ses-2_task-BART_run-9_bold.nii.gz\n",
      "|           |-- sub-REV055_ses-2_task-GNG_run-11_bold.json\n",
      "|           |-- sub-REV055_ses-2_task-GNG_run-11_bold.nii.gz\n",
      "|           |-- sub-REV055_ses-2_task-GNG_run-12_bold.json\n",
      "|           |-- sub-REV055_ses-2_task-GNG_run-12_bold.nii.gz\n",
      "|           |-- sub-REV055_ses-2_task-React_run-15_bold.json\n",
      "|           |-- sub-REV055_ses-2_task-React_run-15_bold.nii.gz\n",
      "|           |-- sub-REV055_ses-2_task-React_run-16_bold.json\n",
      "|           |-- sub-REV055_ses-2_task-React_run-16_bold.nii.gz\n",
      "|           |-- sub-REV055_ses-2_task-SST_run-5_bold.json\n",
      "|           |-- sub-REV055_ses-2_task-SST_run-5_bold.nii.gz\n",
      "|           |-- sub-REV055_ses-2_task-SST_run-6_bold.json\n",
      "|           `-- sub-REV055_ses-2_task-SST_run-6_bold.nii.gz\n",
      "|-- sub-REV074\n",
      "|   |-- ses-1\n",
      "|   |   |-- anat\n",
      "|   |   |   |-- sub-REV074_ses-1_acq-mprage_run-1010_T1w.json\n",
      "|   |   |   `-- sub-REV074_ses-1_acq-mprage_run-1010_T1w.nii.gz\n",
      "|   |   |-- fmap\n",
      "|   |   |   |-- sub-REV074_ses-1_run-14_magnitude1.json\n",
      "|   |   |   |-- sub-REV074_ses-1_run-14_magnitude1.nii.gz\n",
      "|   |   |   |-- sub-REV074_ses-1_run-14_magnitude2.json\n",
      "|   |   |   |-- sub-REV074_ses-1_run-14_magnitude2.nii.gz\n",
      "|   |   |   |-- sub-REV074_ses-1_run-15_phasediff.json\n",
      "|   |   |   |-- sub-REV074_ses-1_run-15_phasediff.nii.gz\n",
      "|   |   |   |-- sub-REV074_ses-1_run-6_magnitude1.json\n",
      "|   |   |   |-- sub-REV074_ses-1_run-6_magnitude1.nii.gz\n",
      "|   |   |   |-- sub-REV074_ses-1_run-6_magnitude2.json\n",
      "|   |   |   |-- sub-REV074_ses-1_run-6_magnitude2.nii.gz\n",
      "|   |   |   |-- sub-REV074_ses-1_run-7_phasediff.json\n",
      "|   |   |   `-- sub-REV074_ses-1_run-7_phasediff.nii.gz\n",
      "|   |   `-- func\n",
      "|   |       |-- sub-REV074_ses-1_task-BART_run-5_bold.json\n",
      "|   |       |-- sub-REV074_ses-1_task-BART_run-5_bold.nii.gz\n",
      "|   |       |-- sub-REV074_ses-1_task-GNG_run-11_bold.json\n",
      "|   |       |-- sub-REV074_ses-1_task-GNG_run-11_bold.nii.gz\n",
      "|   |       |-- sub-REV074_ses-1_task-GNG_run-12_bold.json\n",
      "|   |       |-- sub-REV074_ses-1_task-GNG_run-12_bold.nii.gz\n",
      "|   |       |-- sub-REV074_ses-1_task-GNG_run-13_bold.json\n",
      "|   |       |-- sub-REV074_ses-1_task-GNG_run-13_bold.nii.gz\n",
      "|   |       |-- sub-REV074_ses-1_task-React_run-16_bold.json\n",
      "|   |       |-- sub-REV074_ses-1_task-React_run-16_bold.nii.gz\n",
      "|   |       |-- sub-REV074_ses-1_task-React_run-17_bold.json\n",
      "|   |       |-- sub-REV074_ses-1_task-React_run-17_bold.nii.gz\n",
      "|   |       |-- sub-REV074_ses-1_task-SST_run-8_bold.json\n",
      "|   |       |-- sub-REV074_ses-1_task-SST_run-8_bold.nii.gz\n",
      "|   |       |-- sub-REV074_ses-1_task-SST_run-9_bold.json\n",
      "|   |       `-- sub-REV074_ses-1_task-SST_run-9_bold.nii.gz\n",
      "|   `-- ses-2\n",
      "|       |-- anat\n",
      "|       |   |-- sub-REV074_ses-2_acq-mprage_run-1010_T1w.json\n",
      "|       |   `-- sub-REV074_ses-2_acq-mprage_run-1010_T1w.nii.gz\n",
      "|       |-- fmap\n",
      "|       |   |-- sub-REV074_ses-2_run-13_magnitude1.json\n",
      "|       |   |-- sub-REV074_ses-2_run-13_magnitude1.nii.gz\n",
      "|       |   |-- sub-REV074_ses-2_run-13_magnitude2.json\n",
      "|       |   |-- sub-REV074_ses-2_run-13_magnitude2.nii.gz\n",
      "|       |   |-- sub-REV074_ses-2_run-14_phasediff.json\n",
      "|       |   |-- sub-REV074_ses-2_run-14_phasediff.nii.gz\n",
      "|       |   |-- sub-REV074_ses-2_run-6_magnitude1.json\n",
      "|       |   |-- sub-REV074_ses-2_run-6_magnitude1.nii.gz\n",
      "|       |   |-- sub-REV074_ses-2_run-6_magnitude2.json\n",
      "|       |   |-- sub-REV074_ses-2_run-6_magnitude2.nii.gz\n",
      "|       |   |-- sub-REV074_ses-2_run-7_phasediff.json\n",
      "|       |   `-- sub-REV074_ses-2_run-7_phasediff.nii.gz\n",
      "|       `-- func\n",
      "|           |-- sub-REV074_ses-2_task-BART_run-5_bold.json\n",
      "|           |-- sub-REV074_ses-2_task-BART_run-5_bold.nii.gz\n",
      "|           |-- sub-REV074_ses-2_task-GNG_run-8_bold.json\n",
      "|           |-- sub-REV074_ses-2_task-GNG_run-8_bold.nii.gz\n",
      "|           |-- sub-REV074_ses-2_task-GNG_run-9_bold.json\n",
      "|           |-- sub-REV074_ses-2_task-GNG_run-9_bold.nii.gz\n",
      "|           |-- sub-REV074_ses-2_task-React_run-15_bold.json\n",
      "|           |-- sub-REV074_ses-2_task-React_run-15_bold.nii.gz\n",
      "|           |-- sub-REV074_ses-2_task-React_run-16_bold.json\n",
      "|           |-- sub-REV074_ses-2_task-React_run-16_bold.nii.gz\n",
      "|           |-- sub-REV074_ses-2_task-SST_run-11_bold.json\n",
      "|           |-- sub-REV074_ses-2_task-SST_run-11_bold.nii.gz\n",
      "|           |-- sub-REV074_ses-2_task-SST_run-12_bold.json\n",
      "|           `-- sub-REV074_ses-2_task-SST_run-12_bold.nii.gz\n",
      "`-- sub-REV126\n",
      "    |-- ses-1\n",
      "    |   |-- anat\n",
      "    |   |   |-- sub-REV126_ses-1_acq-mprage_run-1012_T1w.json\n",
      "    |   |   `-- sub-REV126_ses-1_acq-mprage_run-1012_T1w.nii.gz\n",
      "    |   |-- fmap\n",
      "    |   |   |-- sub-REV126_ses-1_run-15_magnitude1.json\n",
      "    |   |   |-- sub-REV126_ses-1_run-15_magnitude1.nii.gz\n",
      "    |   |   |-- sub-REV126_ses-1_run-15_magnitude2.json\n",
      "    |   |   |-- sub-REV126_ses-1_run-15_magnitude2.nii.gz\n",
      "    |   |   |-- sub-REV126_ses-1_run-16_phasediff.json\n",
      "    |   |   |-- sub-REV126_ses-1_run-16_phasediff.nii.gz\n",
      "    |   |   |-- sub-REV126_ses-1_run-8_magnitude1.json\n",
      "    |   |   |-- sub-REV126_ses-1_run-8_magnitude1.nii.gz\n",
      "    |   |   |-- sub-REV126_ses-1_run-8_magnitude2.json\n",
      "    |   |   |-- sub-REV126_ses-1_run-8_magnitude2.nii.gz\n",
      "    |   |   |-- sub-REV126_ses-1_run-9_phasediff.json\n",
      "    |   |   `-- sub-REV126_ses-1_run-9_phasediff.nii.gz\n",
      "    |   `-- func\n",
      "    |       |-- sub-REV126_ses-1_task-BART_run-5_bold.json\n",
      "    |       |-- sub-REV126_ses-1_task-BART_run-5_bold.nii.gz\n",
      "    |       |-- sub-REV126_ses-1_task-BART_run-6_bold.json\n",
      "    |       |-- sub-REV126_ses-1_task-BART_run-6_bold.nii.gz\n",
      "    |       |-- sub-REV126_ses-1_task-BART_run-7_bold.json\n",
      "    |       |-- sub-REV126_ses-1_task-BART_run-7_bold.nii.gz\n",
      "    |       |-- sub-REV126_ses-1_task-GNG_run-10_bold.json\n",
      "    |       |-- sub-REV126_ses-1_task-GNG_run-10_bold.nii.gz\n",
      "    |       |-- sub-REV126_ses-1_task-GNG_run-11_bold.json\n",
      "    |       |-- sub-REV126_ses-1_task-GNG_run-11_bold.nii.gz\n",
      "    |       |-- sub-REV126_ses-1_task-React_run-17_bold.json\n",
      "    |       |-- sub-REV126_ses-1_task-React_run-17_bold.nii.gz\n",
      "    |       |-- sub-REV126_ses-1_task-React_run-18_bold.json\n",
      "    |       |-- sub-REV126_ses-1_task-React_run-18_bold.nii.gz\n",
      "    |       |-- sub-REV126_ses-1_task-SST_run-13_bold.json\n",
      "    |       |-- sub-REV126_ses-1_task-SST_run-13_bold.nii.gz\n",
      "    |       |-- sub-REV126_ses-1_task-SST_run-14_bold.json\n",
      "    |       `-- sub-REV126_ses-1_task-SST_run-14_bold.nii.gz\n",
      "    `-- ses-2\n",
      "        |-- anat\n",
      "        |   |-- sub-REV126_ses-2_acq-mprage_run-1016_T1w.json\n",
      "        |   `-- sub-REV126_ses-2_acq-mprage_run-1016_T1w.nii.gz\n",
      "        |-- fmap\n",
      "        |   |-- sub-REV126_ses-2_run-13_magnitude1.json\n",
      "        |   |-- sub-REV126_ses-2_run-13_magnitude1.nii.gz\n",
      "        |   |-- sub-REV126_ses-2_run-13_magnitude2.json\n",
      "        |   |-- sub-REV126_ses-2_run-13_magnitude2.nii.gz\n",
      "        |   |-- sub-REV126_ses-2_run-14_phasediff.json\n",
      "        |   |-- sub-REV126_ses-2_run-14_phasediff.nii.gz\n",
      "        |   |-- sub-REV126_ses-2_run-19_magnitude1.json\n",
      "        |   |-- sub-REV126_ses-2_run-19_magnitude1.nii.gz\n",
      "        |   |-- sub-REV126_ses-2_run-19_magnitude2.json\n",
      "        |   |-- sub-REV126_ses-2_run-19_magnitude2.nii.gz\n",
      "        |   |-- sub-REV126_ses-2_run-20_phasediff.json\n",
      "        |   `-- sub-REV126_ses-2_run-20_phasediff.nii.gz\n",
      "        `-- func\n",
      "            |-- sub-REV126_ses-2_task-BART_run-15_bold.json\n",
      "            |-- sub-REV126_ses-2_task-BART_run-15_bold.nii.gz\n",
      "            |-- sub-REV126_ses-2_task-GNG_run-11_bold.json\n",
      "            |-- sub-REV126_ses-2_task-GNG_run-11_bold.nii.gz\n",
      "            |-- sub-REV126_ses-2_task-GNG_run-12_bold.json\n",
      "            |-- sub-REV126_ses-2_task-GNG_run-12_bold.nii.gz\n",
      "            |-- sub-REV126_ses-2_task-GNG_run-5_bold.json\n",
      "            |-- sub-REV126_ses-2_task-GNG_run-5_bold.nii.gz\n",
      "            |-- sub-REV126_ses-2_task-GNG_run-6_bold.json\n",
      "            |-- sub-REV126_ses-2_task-GNG_run-6_bold.nii.gz\n",
      "            |-- sub-REV126_ses-2_task-React_run-21_bold.json\n",
      "            |-- sub-REV126_ses-2_task-React_run-21_bold.nii.gz\n",
      "            |-- sub-REV126_ses-2_task-React_run-22_bold.json\n",
      "            |-- sub-REV126_ses-2_task-React_run-22_bold.nii.gz\n",
      "            |-- sub-REV126_ses-2_task-SST_run-17_bold.json\n",
      "            |-- sub-REV126_ses-2_task-SST_run-17_bold.nii.gz\n",
      "            |-- sub-REV126_ses-2_task-SST_run-18_bold.json\n",
      "            `-- sub-REV126_ses-2_task-SST_run-18_bold.nii.gz\n",
      "\n",
      "27 directories, 181 files\n"
     ]
    }
   ],
   "source": [
    "!tree {output_path}/REV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d638e9",
   "metadata": {},
   "source": [
    "This will pass bids validation, but there's still cleanup to do, especially if you're going to use something like fmriprep. You'll want those extra fieldmap related json fields, and the fieldmaps should be renamed to something like run-1 and run-2, and you need to look for aborted runs and take them out. Hopefully this tutorial is enough to get you started with your own conversion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "948d3aa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
